# ðŸ’¼ Nombre del proyecto

<h3 align="center" style=" line-height: 1.375rem; letter-spacing: 0.0091428571em; font-style: italic;">RAG AGAINST THE MACHINE</h3>

## ðŸ“ DescripciÃ³n

Desarrollar e implementar un sistema que facilite a los clientes finales realizar preguntas en lenguaje humano natural respecto a asuntos vinculados con la empresa (y lograr una respuesta razonable). 

## ðŸ“Š GuÃ­a de configuraciÃ³n y ejecuciÃ³n del sistema

  ### ðŸ’» Prerrequisitos
-	Clonar el repositorio.
-	Tener Visual Studio Code instalado.
-	Tener una versiÃ³n de Python 3.12.5 o superior.
-	Instalar Ollama.
-	Instalar Docker.
-	Descargar Telegram Desktop
-	SerÃ¡ necesario tener pip instalado en el sistema para gestionar la instalaciÃ³n de las dependencias requeridas por el proyecto.


## ðŸ’» Pasos para la ejecuciÃ³n del sistema

> [!NOTE]
> Tenemos que situarnos en la ruta correspondiente a cada ejercicio para poder ejecutarlo, sino se producirÃ¡ un error.
Una vez instalado es necesario configurar las dependencias para que cada ejercicio se pueda ejecutar correctamente. Seguidamente y por cada ejercicio desarrollado, se mencionan como instalarse.

### ðŸ’» Ejercicio 1: Parte Obligatoria
1.	Instalar las siguientes dependencias:

-	pip install PyPDF2 
-	pip install langchain
-	pip install langchain-core
-	pip install ollama
-	pip install chromadb
  
Modificar la ruta para cargar los documentos: **variable (pdf_path)** del fichero *ejercicio1.py*

2.	Para reproducir los experimentos realizados tenemos que:
-	Modificar los parÃ¡metros â€œchunk_sizeâ€ y â€œchunk_overlapâ€, para modificar el tamaÃ±o de cada trozo y su solapamiento, en la funciÃ³n â€œsplit_text_into_chunksâ€
-	Modificar tanto el nÃºmero de resultados relevantes (n_results) asÃ­ como al generar la respuesta (temperatura, top_k,num_predict y top_p) y ver cÃ³mo poder simular todo lo que se ha probado para conseguir un resultado Ã³ptimo y que asÃ­ se ha reflejado en la memoria.

Para ejecutarlo por terminal serÃ­a suficiente, con poner: *python3 ejercicio1.py â€œpreguntaâ€*

> [!NOTE]
> En este caso, serÃ¡ necesario:
> - Abrir un terminal y ejecutar: *ollama serve*
> - Abrir un terminal de Docker y ejecutar: *docker run -p 8000:8000 chromadb/chroma*, para iniciar el servidor de **ChromaDB**

### ðŸ’» Ejercicio 2: Parte Obligatoria
1.	Instalar las siguientes dependencias:

Para poder ejecutar este ejercicio necesitamos disponer de la API_KEY de Google, para ello tenemos que ir al enlace: https://ai.google.dev/gemini-api/docs/api-key?hl=es-419 y configurarla. A continuaciÃ³n instalar la dependencias, como se indica a continuaciÃ³n:

-	pip install google-generativeai

Modificar la ruta para cargar los documentos: **variable (pdf_path)** del fichero *ejercicio2.py*

2.	Para reproducir los experimentos realizados tenemos que:

-	Modificar los parÃ¡metros â€œchunk_sizeâ€ y â€œchunk_overlapâ€, para modificar el tamaÃ±o de cada trozo y su solapamiento, en la funciÃ³n â€œsplit_text_into_chunksâ€
-	Modificar tanto el nÃºmero de resultados relevantes (n_results) asÃ­ como al generar la respuesta (temperatura, top_k,num_predict y top_p) y ver cÃ³mo poder simular todo lo que se ha probado para conseguir un resultado Ã³ptimo y que asÃ­ se ha reflejado en la memoria.

Para ejecutarlo por terminal serÃ­a suficiente, con poner: *python3 ejercicio2.py â€œpreguntaâ€*

> [!NOTE]
> En este caso, se necesitarÃ¡:
> - Abrir un terminal de Docker y ejecutar: *docker run -p 8000:8000 chromadb/chroma*

### ðŸ’» Ejercicio 1: Parte Opcional

1.	Instalar las siguientes dependencias:
-	pip install beautifulsoup4
-	pip install langchain-chroma
-	pip install langchain-community

2.	Modificar la ruta **variable (htlm_path)** del archivo *chatbot_telegram.py* para que los mensajes del archivo HTML se carguen correctamente.

4.	Para reproducir los experimentos realizados debemos de:


### ðŸ’» Ejercicio 2: Parte Opcional

1.	Instalar las siguientes dependencias:

-	pip install Transformers
-	pip install pinecone-client

Modificar la ruta para cargar los documentos: **variable (pdf_path)** del fichero *main_pinecone.py*

2.	Para poder ejecutar este ejercicio necesitamos disponer de la *API_KEY* de *Pinecone*, para ello tenemos que ir al enlace: https://docs.pinecone.io/guides/get-started/quickstart  y configurarla en el apartado â€œGet an-API Keyâ€. Con esto ya tendremos funcionando a *Pinecone* como BD vectorial en la nube.

3.	Para reproducir los experimentos realizados debemos de:

-	Modificar los parÃ¡metros â€œchunk_sizeâ€ y â€œchunk_overlapâ€, para modificar el tamaÃ±o de cada trozo y su solapamiento, en la funciÃ³n â€œsplit_text_into_chunksâ€
-	Modificar tanto el nÃºmero de resultados relevantes (n_results) asÃ­ como al generar la respuesta (temperatura, top_k,num_predict y top_p) y ver cÃ³mo poder simular todo lo que se ha probado para conseguir un resultado Ã³ptimo y que asÃ­ se ha reflejado en la memoria.

Para ejecutarlo por terminal serÃ­a suficiente, con poner: *python3 ejercicio2_pinecone.py â€œpreguntaâ€*

### ðŸ’» Ejercicio 3: Parte Opcional
1.	Instalar las siguientes dependencias:

-	pip install langchain-google-genai
-	pip install Pillow
-	pip install easyocr

Proporcionar una ruta vÃ¡lida para la **variable (image_path)**, en el archivo *ejercicio2_versionimg* que haga referencia a un archivo de imagen en el sistema del usuario, es decir que ese archivo exista en la ubicaciÃ³n especificada. AdemÃ¡s la imagen debe estar en un formato que *EasyOcr* pueda leerlo (como .jpg, .png, etcâ€¦)

> [!NOTE]
> El usuario necesitarÃ¡ tener permisos de lectura para el/los archivo/s de imagen.

2.	Para reproducir los experimentos realizados debemos de:
